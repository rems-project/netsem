Trace-checking scripts
~~~~~~~~~~~~~~~~~~~~~~

This directory contains the scripts that are used to drive the trace
checker, validating that the set of traces does indeed match the
specification.

Documentation rewritten 2004-09-20,21 by Keith Wansbrough.




======================================================================

CONTENTS
~~~~~~~~

Quick start.
Analysis.
Checking internals.
Miscellaneous.
Recipes.




======================================================================

QUICK START
~~~~~~~~~~~

All you need to know to drive the system.

== CONFIGURATION:

First, construct a configuration file specifing the correct parameters
for the run.  In particular, be sure that TRACES is set to a
space-separated list of all the traces you want to check, in the order
in which you want them to be checked, and that PTCMACHINES is set to a
list of all the machines to be used and their machine-specific
configuration.

The default configuration file is runbatch.conf, and it includes a
machine configuration machines.conf.  The configuration file can have
any name you like.  There are helper scripts that create configuration
files for specific purposes; see bsd_non-succeeded-small-first for
example.

The machines configuration file machines.conf is usually
auto-generated by genmachines; edit variables near the top of this
script to change parameters, including the pool of machines to select
from, and the minimum requirements (speed and memory) for a machine to
be selected.


== STARTING:

Invoking startbatch with no arguments will start a run using the
default configuration file.  Giving it "-f configfile" will use the
named configuration file.  In either case, you will be asked for a
"reason" for the run; this is a one-line description of what the run
is for, which is included in the email notification sent to all users
listed in MAILTO (in the config file).  startbatch will return
immediately.

Once it has returned, you should monitor its progress immediately (to
be sure it started successfully and didn't encounter a permissions or
syntax problem during startup), and again over the first hour or two
to be sure everything is behaving as expected.  It can then be left to
run to completion or until it is desired to stop.  A typical run may
take from 24 hours up to a couple of weeks.


== MONITORING PROGRESS:

When a run is started, a new directory is created, of the form
/usr/groups/tthee/check/check-<timestamp>/, and a link is created to
it from /usr/groups/tthe/check/current.  All check output is placed in
this directory.

Watch the tail of status.dat (tail -f status.dat, or less status.dat
and then hit shift-F) during startup to confirm that each machine in
turn starts up correctly (there is a several-second delay between
starting each worker, in order to reduce load on the file server) and
doesn't immediately crash (go into CantStart).

If there's a problem, look at log.dat (or possibly log-run.dat) to see
the error messages.

Watch index-re.html to see the progress of the run as it goes.

Click on an individual trace number to see the results for a
particular trace; click on various things to expand the view (many
things are hidden in the default view).

Note that all HTML generated by tools in this project is known to work
with recent versions of Mozilla Firefox.  It complies with the various
W3C standards, and so is unlikely to work with standards-violating
browsers such as Microsoft's Internet Explorer.

log-run.dat      The start and stop reasons are logged to this file.

log.dat          The behaviour of the check-run driver otracechecker,
                 and of all the workers doing the actual checking, is
                 logged to this file.  Several other programs analyse
                 this log to obtain various information about the run,
                 and so it is designed to be machine-readable (via
                 lines beginning with "==") as well as human-readable.

status.dat       Every time the state of the driver or some worker
                 changes, a summary of the current state is appended
                 to this file.  Only the last entry is actually of
                 interest.  This file is particularly useful as a
                 "dashboard" showing the current state of the checking
                 run.  In particular, it shows which machines are in
                 use and which are idle, which have crashed and been
                 abandoned (CantStart), and how many traces each
                 machine has processed.  This file is intended for
                 human consumption only.

trace*.out.html  As each trace is checked, the results of checking are
                 written to a corresponding HTML file.  When a trace
                 is completed, the end of this file records whether it
                 succeeded, failed, or terminated for some other
                 reason.  This HTML file uses cascading style sheets
                 (CSS), ECMAScript, and DOM, to make the large volume
                 of data accessible.  In particular, each transition
                 is collapsed under its label; click on the label
                 (yellow rectangle) and the transition itself will be
                 revealed below.  Furthermore, introduction of
                 variables and constraints is normally hidden;
                 clicking the relevant text near the top of the page
                 will reveal it (on a blue background).  Hyperlinks
                 from this document back to the original trace also
                 appear, in either highlighted, plain, or
                 Postscript-rendered forms.

index.html       A summary of the progress of checking, the results of
                 each trace so far, and links to the detailed trace
                 results pages, is given here.  This page is
                 regenerated every few minutes, but may lag some time
                 behind the current results as the analysis takes some
                 time to run.  Should be fairly self-explanatory; note
                 that clicking on the [D] displays the long
                 description in a dialog box.  It also contains a
                 command for invoking stepgraph (q.v. below).

index-re.html    A version of the above that is set to automatically
                 reload frequently.

trace.css        Support files (style sheets and scripts) for the trace
trace.js         output.


== STOPPING:

In theory, when all traces have been processed, the run will stop
itself and clean up, emailing the list in MAILTO that it has completed
normally.

In practice, there are always a few long-running traces that don't
look like completing, or we have a reason to stop the current run and
replace it with another.  In these circumstances, we must stop the
current run.

To do this, simply invoke "stopbatch" from the machine that was used
initially to invoke "startbatch".  This is the machine that is
controlling the run; it is named in the email notification sent out at
start.

You will be presented with a menu of current runs; choose the relevant
one, and when asked, give a reason for stopping.  This will be
included in the email notification of termination sent out.  The run
will then be stopped.

Note that stopping the run causes all the workers to appear to crash
from the point of view of the logging tools (log.dat, status.dat),
with LocalCrash or CantStart.  This is normal.

It is good practice to wait a minute or two before starting another
run, to be sure that asynchronous cleanup processes have time to do
their work.




======================================================================

ANALYSIS
~~~~~~~~

== analyselog [-r] <logfile>

Analyse the logfile (log.dat) specified on the command line, and write
a report to stdout.  This analysis is from the point of view of
otracechecker; it ignores the actual results (success/failure/...) of
each trace, and simply considers the worker behaviour.

At any point in time, each worker is in a certain state
(WaitingUntilExecSafe, Starting, WaitingForWork, Processing,
Completed, etc).  This program builds up a state transition diagram,
examining the time of each state change and recording, for each
state->state transition, how many times it has occurred, and the
average time spent in the first state before transitioning to the
second.  Pseudo-states "0" (initial) and "1" (final) are added.

The report is slowly being extended, but at the moment it gives:

- a detailed analysis of the state transition diagram for each worker.
This reveals useful information about the speed of the worker, and
also about the relative costs of each phase of processing, as well as
what each worker was doing at the point the log ends.

- a summary of the relative speeds of each worker, computed on the
basis of the startup time (which is a fixed piece of work, as opposed
to the traces which vary widely).

- a summary of the speed and number of attempts on each job (trace).


== prettytrace

Generates "pretty" (HTMLised) versions of all traces in the current
directory, and an index.  NB: processes *trace* files, not
check-output files.

This is run in each /usr/groups/tthee/batch/autotest*/ directory after
the trace set is generated.

Prettification is simply highlighting different labels in different
colours.


== regress {-t <tracedir>} ... <checkdir> ...

Analyse multiple directories of traces to be checked, and multiple
check runs, outputting a grid showing the results of checking each
trace each time.  Useful for regression testing - does a trace ever
fail, after having once succeeded?

Results are output to /usr/groups/tthee/check/regression.html and
/usr/groups/tthee/check/regression-warnings.html.

The script uses the test descriptions to identify which tests are the
same, independent of their trace number.  If there are duplicate
descriptions, a warning is generated and something appropriate is
done.

This takes a fair while to run, and the resulting grid is pretty big.


== stepgraph <tracefoo.out.html>  / stepgraph_p <tracefoo.out.html>

Plots a graph showing the backtracking behaviour of a trace.  The
horizontal axis is the steps taken by the checker, and the vertical
axis is the step number in the source trace.  This is very useful for
knowing where to look in a long check-output file when debugging.

Note that clicking the [D] in the trace-checker index page gives a
dialog containing a ready-written stepgraph invocation.  Triple-click
the edit box, and then paste into an xterm.

The _p variant generates Postscript, rather than an X11 graph.


== timeanal <tracefoo.out.html> ...

Analyse a trace output file, displaying the stated time spent
performing each transition, ranked in order of time.


== logpauses <tracefoo.out.html>

A very simple script that logs the length of the given file in
tracefoo.out.html.size-log every 10 seconds or so.  This is intended
to be used in parallel with a check run, to obtain data about where
the checker is spending its time.


== dop </usr/groups/tthee/batch/autotest-tcp-foo/trace0123>

A little helper script for logpauses.  Runs CheckTraces and logpauses
together:

cd /usr/groups/tthee/local/Net/TCP/Spec1/
../Test/scripts/check/dop /usr/groups/tthee/batch/autotest-tcp-2004-10-01T16\:28\:00+0100/trace1138
ls -l trace*
cat dop-log.*

== procpauses <tracefoo.out.html> <tracefoo.out.html.size-log>

Analyses the data collected by logpauses, and prints a summary of
where the checker is spending its time.  Decides two points are "the
same" if the three characters before the points are identical; prints
at the end of each line the longest common suffix leading up to those
points, to provide more context.


== progressdata > foo.ps

When run in a check-output directory, generates a Postscript graph
showing the number of traces in the various classes as a function of
real time.


== inprogressreport

When run in a check-output directory, generates a list of all the
traces currently in progress, with the time they have been running,
the size of the output file, the time since the output file was last
updated ("idle time"), and the machine on which each is running.  This
is intended to be useful to discover "runaway" traces that are ripe
for being killed (please use SIGUSR1 to avoid confusion with internal
signals).


== timesgraph

When run in a check-output directory, on the result of mktraceindex --simple,
eg

stem:current$ ~/Net/TCP/Test/scripts/check/mktraceindex --simple  --only=SUCCEEDED > simple-succ.dat
stem:current$ ~/Net/TCP/Test/scripts/check/timesgraph simple-succ.dat

this generates two postscript histograms, timesgraph.real.ps and
timesgraph.st.ps, from that .dat file. It also prints out the count
and mean on standard error.

(mktraceindex can be given an option to pick out just the succeeded
traces if you want)




======================================================================

CHECKING INTERNALS
~~~~~~~~~~~~~~~~~~

== startbatch [ -f conffile ]

Inputs the configuration file, obtains a reason for starting, and
invokes runbatch.


== runbatch.conf

The default configuration file.

Variables that must be set in the configuration file:

TEST       =/bin/false
GZIPLOG    =/bin/false
MAILTO     Space-separated list of people to send email notifications
           to.
FIFOGROUP  Unix group of people allowed to stop the run.
FIFOPERM   Unix permissions for the file that is used to stop the run
           (should probably be 660, for "user and group only").
INDIR      Directory from which traces are taken.
TRACES     Space-separated list of full paths of traces.  Note that
           this variable is usually too big to pass in the environment;
           it must be set inside the same bash script as it is used.
           "$INDIR/trace????" is a typical value.
BINDIR     Directory in which this script, and the other helper scripts
           for the run, are found.  Must be valid for all machines
           in the run.
SPECDIR    Directory in which the worker (CheckTraces.exe) is found.
           Must be valid for all machines in the run.
OUTDIR     Directory in which output will be placed; will be created
           if not present.  Usually involves "`date -Isec`" for
           uniqueness.
PTCARGS    Arguments to otracechecker (see below).  Often includes
           PTCMACHINES, the machine configuration arguments, which
           are often set in machines.conf.


== machines.conf

Included by typical configuration files; a fragment of the argument
string to otracechecker, specifying the machines to be used in the
run.  Syntax specified under otracechecker below.

Often generated by genmachines.


== genmachines

A script to generate a machines.conf.  Should be self-explanatory;
edit the variables near the top of the file to change the machine pool
and the required specification, etc.


== runbatch conffile "reason"

Reads the config file and the reason, and actually starts up the run.

Specifically:

Creates the output directory.  Logs the start, and sends an email
message to all concerned.  Starts a process to repeatedly invoke
mktraceindex.  Starts otracechecker with the appropriate arguments,
passing the list of traces to stdin since it is usually too big for
the command line (on Linux this has a 256K limit[*]).  Sets up a signal
handler for killing.  Creates a FIFO, and sets up a reader process
that waits for a message to appear on the FIFO, and then (using that
message as a reason) kills the check run.  Waits for completion, then
removes the FIFO, runs mktraceindex one last time in the output
directory, logs, and mails all interested parties.

[*] the limit is around 256K I think - and it's reasonable because all
the args have to fit on the kernel *stack* while the process they were
in is destroyed.  That's quite a lot to stick on a stack.


== stopbatch

Presents a menu of all the FIFOs available (i.e., currently-executing
check runs).  On choosing one, obtains a reason, and writes it to the
FIFO.  This should have the effect of causing the job to commit
suicide.  Note that it is likely that FIFOs do not have the desired
behaviour across a network; you probably should run stopbatch on the
machine that was used to invoke startbatch (as detailed in the email
notification sent out at that time).


== mktraceindex [--sort=class] [--only=<CLASS>] [--ntraces=<n>]

Expects to be run in the check output directory
(/usr/groups/tthee/check/check-<date>/); specifically, expects to see
"./log.dat" and "./*.out.html".

Outputs an HTML summary of state of traces so far to stdout.  Note
that this is slightly incomplete; runbatch normally runs sed over this
to create two versions, index.html and index-re.html (see the source
code of runbatch for details).

--sort=class requests that traces be presented in order of class
(e.g., SUCCESS, FAILED, CRASHED, ...) and only then in order of trace
number (the default is by trace number).

--only=<CLASS> requests that only traces with the given class are
displayed.

The fraction of total traces done so far will be incorrect (or
possibly missing) unless the --ntraces option is specified, giving the
total number of traces to be processed (this information is not
otherwise available in the check directory, as we see only traces that
have been checked at least partially).

For full details, examine the output and the source code.

Note in particular that this program uses two local Perl modules,
Netsem::Util and Netsem::LogParse.  Some Perl hackery means that it
finds these in the Netsem/ subdirectory of the directory in which the
program itself is located; it might be better to have them in some
standard path, but this would require configuration of the path for
everyone who might use the program.  The present method is at least
self-contained.


== Netsem/Util.pm

A helper Perl module, containing (so far) only a single function:
relpath, which computes the relative path from one path to another.
This is used to ensure that paths in index.html are relative;
essential when accessing it both from the local file system and
remotely via HTTP.


== Netsem/LogParse.pm

A helper Perl module, which draws together all the code necessary for
parsing information from a log file.  The code should be relatively
well documented, if a little un-pretty.


== otracechecker <options> <file> ...

The check run coordinator.  This takes a specification of the traces
and the workers, and assigns the traces to the workers, monitoring and
logging their progress, restarting workers and retrying traces as
necessary.  Command-line arguments are specified in the usage message;
at the time of writing they were:

  -j[[<rsh>:]<hostname>:]<nproc>[=<inuse>]
                        Use <nproc> worker threads, locally or
                          on <hostname>, with <inuse> as the
                          inuse-detection program if any,
                          and using the specified rsh program
                          if given.  (extra specs add more workers)
  -m<n>                 Maximum number of files to process with
                          a single worker instance (-1 for unbounded)
  -c<cmd> <arg> ... \;  Worker command to run
  -f<file>              Read files to process from <file>, separated
                          by whitespace.  Use - for stdin
  -s<file>              Write status reports to <file> rather than stdout
  -y                    Use dYnamic mode to communicate filenames
                          to workers (worker command must use the
                          simple accept protocol)
  -w<n>                 Minimum time between execs (in seconds)
  -z<cmd> <arg> ... \;  Command to invoke for an aborted file;
                          passed filename and abort type
  -r<cmd> <arg> ... \;  RSH/SSH command to run for remote workers
  <file>                Process <file>

Log information (log.dat) is written to stdout; status information is
written to the file specified by -f (status.dat) (see above for the
contents of these files).

The algorithm is a standard worklist algorithm; the workers are
started, and a queue of traces is created; as each worker becomes
ready, it is assigned a trace to work on; as each trace is completed
and the worker becomes free, new work is assigned it; this continues
until there are no traces left and all workers have terminated.

Workers are started sequentially, with a short delay, to reduce load
on the file server.  If a worker crashes during startup, it is assumed
to have a fault, and is not used again.  If a worker crashes while
processing a trace, it is restarted.  If a worker is killed, it is
restarted and the trace (if any) is scheduled to be retried (it is
however placed at the end of the queue rather than the front).

Provision is made for an "inuse script" to declare that a machine is
in use, and should not be used for trace checking.  This is important,
because many of our cycles have been donated, and we want to minimise
the impact on donors.  The current implementation is as follows: when
a machine becomes in use, whatever it is currently doing is killed,
and we wait until it becomes not in use.  We then start the worker
again.  This is a crude but effective implementation; in future we
would like to suspend the current job rather than kill it, so as not
to waste both the startup and the trace-checking time as at present.

Note that -m is ignored, and -y is compulsory - we support only the
dynamic protocol where a worker asks (on stdout) for a trace to
process, we provide the name of one (on stdin), the worker processes
it and announces completion, and repeats.  A former implementation
supported a static mode where traces were passed (in -m-sized blocks)
on the command line.  This approach had a number of deficiencies.

Current arguments from a typical runbatch.conf are:

  -jalfex.cl.cam.ac.uk:2=/usr/groups/tthee/local/Net/TCP/Test/scripts/check/inuse2.pl
  -jastrocyte.cl.cam.ac.uk:1=/usr/groups/tthee/local/Net/TCP/Test/scripts/check/inuse2.pl
  [..]

A two-processor and a one-processor machine, using our inuse2.pl
script, which allows the workstation owners to specify when we may use
their machine by day and time.

  -jakan:1=/homes/pb/pl/inuse.pl
  -jbasa:1=/homes/pb/pl/inuse.pl

Two one-processor machines using the Computing Officer's inuse.pl
script, which determines in-use-ness by whether another user is logged
in, or it is a scheduled time for a practical (these machines are in a
teaching lab).

The inuse script is passed as a "-i" argument to the worker.

  -y

Use dynamic mode (otracechecker communicates with the workers by a
simple protocol, rather than on the command line).

  -w10

Wait at least 10 seconds between starting successive workers.
CheckTraces.exe is currently around 12MB, and is all that is loaded on
startup; on a 100Mb Ethernet this is around 1 second, but on a 10Mb
Ethernet it is around 10 seconds.

  -r /usr/bin/ssh -ttx -e none -oPreferredAuthentications=publickey -oCiphers=blowfish-cbc ;

When communicating with remote workers (it is also possible to use the
local machine as a worker, in which case the -r argument is not used),
we use ssh, requesting that a pty be allocated (so we can kill the
remote process by killing the ssh, which is clearly a requirement), no
X forwarding be done, no escapes are used, and the specified
authentication method and cipher are used.

  -m60

Archaic; ignored.

  -s $OUTDIR/status.dat

Status reports go to this file.

  -c $BINDIR/wrapper -l 160000 -p 19 -b $SPECDIR $SPECDIR/CheckTraces.exe -d $OUTDIR -bt 100% -a ;

The worker that is run on each machine is in fact a wrapper around the
real worker; see 'wrapper' for more details.

  -z $BINDIR/appendtrailer $OUTDIR ;

If a job is killed or crashes, run appendtrailer to add a
correctly-formatted tail to the output.

  -f-

Trace names are passed on stdin (since they are usually too big for
the command line).  Note that they are read all at once, not
incrementally.  In future we would like a more dynamic setup, where
traces and workers can be added or removed on the fly.


== wrapper [-b <LIBPATH>] [-l <MEMLIMIT_kilobytes>] [-p <PRIORITY>] [-i <INUSESCRIPT>]

Sets various things in the environment.  In particular:

- LD_LIBRARY_PATH must be set for CheckTrace.exe to be able to find
certain libraries (not sure if this is still needed).

- the virtual memory size is limited in order to abort runaway
processes before they kill the machine. (we usually use 160M).

- the worker is "niced" to reduce its scheduling priority (we use 19,
the lowest possible priority, to favour the user).

- the inuse script (passed on to wrapper.pl).

There is no portable/easy way of setting these in Perl, so we leave
this wrapper in shell.


== wrapper.pl [-i <INUSESCRIPT>] <prog> <arg> ...

Manage the inuse script while running the worker.

If no inuse script, simply exec()s the program with its arguments.

Otherwise, start both the inuse script and the worker in parallel,
with the inuse script waiting for the machine to become in use.  If it
does, kill the worker, and wait until the machine becomes free before
returning (this ensures that the checker does not use this machine
until it is free).  Write various informative messages to stdout.  Log
return codes and so on.

This script is written in Perl, as it would be complicated and hard to
write and understand if written in shell.  It is unpleasant but I
think unavoidable that the wrapper is in two parts.


== appendtrailer <outdir> <tracefilename> <reason>

See above.  Appends the correct closing to a trace file that was
abruptly terminated.


== /homes/pb/pl/inuse.pl

The Computing Officer's inuse script (see above for description).
Should be self-documenting; try --help.


== inuse2.pl

Our inuse script (see above for description).  Should be
self-documenting; try --help.  Furthermore, see
/usr/groups/tthee/www-internal/timedonors.html for an explanation.  A
machine is controlled by a config file called
/usr/groups/tthee/conf/<machinename>.<domain>, which looks like this:

  tz :Europe/London
  # don't allow use between 1000 and 1959:
  deny * 10-19 * * *
  # allow use any time Sun or Sat:
  allow * * * * 0,6

The script has to do some fun stuff to allow the config file to change
the timezone (TZ) dynamically, given that glibc caches the timezone
info and never bothers to check if TZ has changed (ugh).




======================================================================

MISCELLANEOUS
~~~~~~~~~~~~~

== dummyclient

For testing otracechecker.  This is a dummy client that speaks the
same dynamic protocol as CheckTraces.exe, but "processes" a job much
faster (it does no actual work).  It incorporates randomness to help
ensure that all paths are exercised.


== ocheck / ocheck2

Simple test scripts used in the development of otracechecker.


== hotgzip / hotgunzip

These are online gzip tools, rather like gzip -c and gunzip -c except
that they try much harder to be unbuffered.  This means that they are
suitable for use in a pipeline where we still want a tail -f to give
live information.  They were written to be used for the log and status
information, but in fact were never used for this.


== kill-all-checkers [-<signal>]

What it says.  This should not be necessary with the present
otracechecker.  It should be run by the user who started the check
run.  Optionally, the signal (e.g. -TERM, -CONT) may be specified.


== hsholed

"Huge-security-hole dæmon".  Documentation in the file.  A kind of rsh
server, guarded by a shared secret.  For use in scenarios where
ordinary ssh/rsh cannot be used, e.g., the Intel teaching lab at the
CL.  We haven't yet used this, but the script is developed here.
Decided that huge-security-holed was a bad name to appear in "top", so
renamed it.


== hshole <host> <port> <arg> ...

"Huge-security-hole".  Documentation in file.  A kind of rsh client,
guarded by a shared secret.  See hsholed above.  This is the client half.


== starthsholed

Machines in the Intel teaching lab are named
"pccl000.somethingorother", for various values of 000.  It is possible
to ssh from them to the outside world, but not vice versa.  The
proposed scheme for using them is as follows: log into each, and ssh
from there to one of our machines in the CL (thalamus).  Use this
connection to set up a tunnel in the opposite direction, which is
connected locally to an instance of hsholed.  Now otracechecker can
simply connect to the thalamus end of the tunnel using
drivehshole/hshole (with the shared secret), and do what is required.

Some details are yet to be worked out.

This script is a single, argument-free way of starting the daemon and
establishing the tunnel.  In particular, it chooses the thalamus-end
port number in a standard way based on the machine's hostname.


== drivehshole <remotehost> <arg> ...

Uses the convention above to detemine which port to connect to, and
then uses that port to connect (via the tunnel) to the named machine,
passing <arg> ... to hshole to be executed on the remote.


== arch_first
== bsd_non-succeeded-0small-first
== bsd_small_first
== every_n
== notcomplete
== notdone

Various tools used to generate new runbatch configuration files.


== compress-results <dir> ...

For each directory listed, uses gzip to compress every file in it.
Not sure what this is for.


== xmas_join

Join multiple runs into a single run, giving priority to successful
results when there are overlaps.  Makes a passable log.dat
simply by concatenating all the individual log.dat files (obviously
this gives bogus values for many fields).  Configuration is at the top
of the file, and should be easily understood.


======================================================================

RECIPES
~~~~~~~

== to follow the progress of the run:

cd /usr/groups/tthee/check/current
tail -c +0 -f log.dat | egrep '>?=='

This gives you a live-updating (every second) trace of everything that
happens, starting from the beginning of the run and continuing until
you stop it.  Nice to see that things are continuing.

== to follow a log or status file (i.e., see appends as they happen)

less log.dat
# then press Shift-F to "follow" the file




======================================================================

End of document.

